return('North.America')
}else if (ctry %in% Europe){
return('Europe')
}else if (ctry %in% Latin.and.South.America){
return('Latin.and.South.America')
}else{
return('Other')
}
}
adult$country <- sapply(adult$country,group_country)
adult$country <- factor(adult$country)
adult$marital <- factor(adult$marital)
adult$type_employer <- factor(adult$type_employer)
adult[adult == '?'] <- NA
adult[adult == ' ?'] <- NA
adult$country <- factor(adult$country)
adult$marital <- factor(adult$marital)
adult$type_employer <- factor(adult$type_employer)
adult <- na.omit(adult)
adult$occupation <- sapply(adult$occupation,factor)
adult$income <- factor(adult$income)
adult <- rename(adult,region=country)
set.seed(101)
split <- sample.split(adult$income, SplitRatio=0.7)
train <- subset(adult, split == TRUE)
test <- subset(adult, split == FALSE)
model <- glm(income ~ ., family=binomial(logit), data = train)
test$predicted.income <- predict(model, newdata=test, type='response')
table(test$income, test$predicted.income > 0.5)
summary(model)
install.packages("ISLR")
library(ISLR)
Caravan
str(Caravan)
summary(Caravan$Purchase)
any(is.na(Caravan))
purchase <- Caravan[,86]
std.Caravan <- scale(Caravan[.-86])
std.Caravan <- scale(Caravan[,-86])
test.index <- 1:1000
test.data <- std.Caravan[test.index,]
test.purchase <- purchase[test.index]
train.data <- std.Caravan[-test.index,]
train.purchase <- purchase[-test.index]
library(class)
set.seed(101)
predict.purchase <- knn(train.data, test.data, train.purchase, k=1)
head(predict.purchase)
error <- mean(test,purchase != predict.purchase)
error <- mean(test.purchase != predict.purchase)
error
library(ISLR)
str(iris)
head(iris)
str(iris)
new.iris <- scale(iris)
new.iris <- scale(iris[])
new.iris <- scale(iris[1:4])
var(new.iris[,1])
iris.species <- iris[,5]
new.iris <- rbind(new.iris,iris.species)
head(new.iris)
new.iris <- cbind(new.iris,iris.species)
head(new.iris)
library(ISLR)
head(iris)
str(iris)
iris.species <- iris[,5]
new.iris <- scale(iris[1:4])
var(new.iris[,1])
new.iris <- cbind(new.iris,iris.species)
head(new.iris)
head(iris)
iris.species <- iris[,5]
iris.species
Species <- iris[,5]
help(cbind)
library(ISLR)
head(iris)
str(iris)
Species <- iris[,5]
new.iris <- scale(iris[1:4])
var(new.iris[,1])
new.iris <- cbind(new.iris,Species)
head(new.iris)
help(cbind)
library(ISLR)
head(iris)
str(iris)
new.iris <- scale(iris[1:4])
var(new.iris[,1])
new.iris <- cbind(new.iris,iris[5])
head(new.iris)
iris[5]
iris[,5]
iris[5,1]
iris[1,1]
iris[1,5]
library(ISLR)
head(iris)
str(iris)
new.iris <- scale(iris[1:4])
var(new.iris[,1])
new.iris <- cbind(new.iris,iris[5])
head(new.iris)
#Train test split
library(caTools)
data <- new.iris.split(new.iris$Species, SplitRatio=0.7)
sample <- sample.split(new.iris$Species, SplitRatio=0.7)
head(sample)
new.iris[-Species,]
new.iris
new.iris[-Species]
new.iris[,-Species]
new.iris[1:4
new.iris[1:4]
new.iris[1:4]
sample <- sample.split(new.iris$Species, SplitRatio=0.7)
train <- subset(new.iris, sample==TRUE)
test <- subset(new.iris, sample==FALSE)
#using KNN
predict <- knn(train[1:4],test[1:4],train$Species,k=1)
library(class)
predict <- knn(train[1:4],test[1:4],train$Species,k=1)
predict
mean(test$Species != predict)
test$Species
error.rate <- NULL
k.values <- 1:10
for(i in k.values){
set.seed(101)
predict <- knn(train[1:4],test[1:4],train$Species,k=i)
error.rate <- mean(test$Species != predict)
}
error.df <- data.frame(error.rate,k.values)
ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()+ geom_line(lty="dotted",color='red')
library(ggplot2)
ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()+ geom_line(lty="dotted",color='red')
train[1:4]
test[1:4]
train$Species
set.seed(101)
#Train test split
library(caTools)
sample <- sample.split(new.iris$Species, SplitRatio=0.7)
train <- subset(new.iris, sample==TRUE)
test <- subset(new.iris, sample==FALSE)
#using KNN
library(class)
predict <- knn(train[1:4],test[1:4],train$Species,k=1)
mean(test$Species != predict)
#choose K
error.rate <- NULL
k.values <- 1:10
for(i in k.values){
set.seed(101)
predict <- knn(train[1:4],test[1:4],train$Species,k=i)
error.rate <- mean(test$Species != predict)
}
error.df <- data.frame(error.rate,k.values)
library(ggplot2)
ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()+ geom_line(lty="dotted",color='red')
mean(test$Species != predict)
predict <- NULL
error.rate <- NULL
k.values <- 1:10
for(i in k.values){
set.seed(101)
predict <- knn(train[1:4],test[1:4],train$Species,k=i)
error.rate <- mean(test$Species != predict)
}
error.df <- data.frame(error.rate,k.values)
library(ggplot2)
ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()+ geom_line(lty="dotted",color='red')
predict <- NULL
error.rate <- NULL
k.values <- 1:10
for(i in k.values){
set.seed(101)
predict <- knn(train[1:4],test[1:4],train$Species,k=i)
error.rate[i] <- mean(test$Species != predict)
}
error.df <- data.frame(error.rate,k.values)
library(ggplot2)
ggplot(error.df,aes(x=k.values,y=error.rate)) + geom_point()+ geom_line(lty="dotted",color='red')
predict <- knn(train[1:4],test[1:4],train$Species,k=2)
mean(test$Species != predict)
predict <- knn(train[1:4],test[1:4],train$Species,k=8)
mean(test$Species != predict)
predict <- knn(train[1:4],test[1:4],train$Species,k=3)
mean(test$Species != predict)
install.packages('rpart')
library(rpart)
help('rpart')
str(kyphosis)
head(kyphosis)
tree <- rpart(Kyphosis ~ . , method='class', data=kyphosis)
printcp(Tree)
printcp(tree)
plot(tree,uniform = T, main='Kyphosis Tree')
text(tree,user.n=T,all=T)
text(tree,use.n=T,all=T)
install.packages('rpart.plot')
prp(tree)
library(rpart.plot)
prp(tree)
install.packages('randomForest')
library(randomForest)
rf.model <- randomForest(Kyphosis ~ . , data=kyphosis)
print(rf.model)
library(ISLR)
head(college)
head(College)
df <- College
head(df)
library(ggplot2)
help(geom_point)
help(ggplot)
help(ggplot)
pl <- ggplot(data=df,aes(x=Room.Board, y=Grad.Rate)) + geom_point(colour=aes(Private))
pl
pl <- ggplot(data=df,aes(x=Room.Board, y=Grad.Rate)) + geom_point(colour=aes(df.Private))
pl
pl <- ggplot(data=df,aes(x=Room.Board, y=Grad.Rate)) + geom_point(colour=Private)
pl <- ggplot(data=df,aes(x=Room.Board, y=Grad.Rate, colour=Private)) + geom_point()
pl
pl <- ggplot(data=df,aes(x=F.Undergrad, colour=Private)) + geom_histogram()
pl
pl <- ggplot(data=df,aes(x=F.Undergrad, fill=Private)) + geom_histogram()
pl
pl <- ggplot(data=df,aes(x=F.Undergrad, fill=Private, colour='black')) + geom_histogram()
pl
pl <- ggplot(data=df,aes(x=F.Undergrad, fill=Private, color='black')) + geom_histogram()
pl
pl <- ggplot(data=df,aes(x=F.Undergrad, fill=Private)) + geom_histogram(color='black'
pl
pl <- ggplot(data=df,aes(x=F.Undergrad, fill=Private)) + geom_histogram(color='black')
pl
pl <- ggplot(data=df,aes(x=Grad.Rate, fill=Private)) + geom_histogram(color='black')
pl
df[Grad.rate>100]
df[Grad.Rate>100]
df[,Grad.Rate>100]
df[df[Grad.Rate]>100]
df[df[,Grad.Rate]>100]
filter(df, Grad_Rate > 100)
filter(df, Grad.Rate > 100)
subset(df, Grad.Rate > 100)
df['Cazenovia College','Grad.Rate'] <- 100
subset(df, Grad.Rate > 100)
library(caTools)
sample <- sample.split(df.Private, SplitRatio = 0.7)
sample <- sample.split(df$Private, SplitRatio = 0.7)
train <- subset(df, sample==T)
test <- subset(df, sample==F)
library(rpart)
tree <- rpart(Private ~ . , data=df, method='class')
predict <- predict(tree,test)
head(predict)
predict <- as.data.frame(predict)
joined <- function(x){
if(x > 0.5){
return("Yes")
}else{
return("No")
}
}
predict$Private <- sapply(predict$Yes,joiner)
joiner <- function(x){
if(x > 0.5){
return("Yes")
}else{
return("No")
}
}
predict$Private <- sapply(predict$Yes,joiner)
head(predict)
table(predict$Private, test$Private)
library(rpart.plot)
help(rpart.plot)
rpart.plot(predict)
prp(predict)
rary(rpart.plot)
library(rpart.plot)
prp(predict)
prp(tree)
library(randomForest)
help(randomForest)
rf.model <- randomForest(Private ~ . , data=train, importance=T)
if(is.matrix(x)){
}
rf.model$confusion
rf.model$importance
p <- predict(rf.model,test)
table(p,test$Private)
library(ISLR)
head(College)
df <- College
head(df)
#EDA
library(ggplot2)
pl <- ggplot(data=df,aes(x=Room.Board, y=Grad.Rate, colour=Private)) + geom_point()
pl
pl <- ggplot(data=df,aes(x=F.Undergrad, fill=Private)) + geom_histogram(color='black')
pl
pl <- ggplot(data=df,aes(x=Grad.Rate, fill=Private)) + geom_histogram(color='black')
pl
subset(df, Grad.Rate > 100)
df['Cazenovia College','Grad.Rate'] <- 100
# Train Test Split
library(caTools)
sample <- sample.split(df$Private, SplitRatio = 0.7)
train <- subset(df, sample==T)
test <- subset(df, sample==F)
# Decision Tree
library(rpart)
tree <- rpart(Private ~ . , data=train, method='class')
predict <- predict(tree,test)
predict <- as.data.frame(predict)
joiner <- function(x){
if(x > 0.5){
return("Yes")
}else{
return("No")
}
}
predict$Private <- sapply(predict$Yes,joiner)
head(predict)
table(predict$Private, test$Private)
library(rpart.plot)
prp(tree)
#Random Forest
library(randomForest)
rf.model <- randomForest(Private ~ . , data=train, importance=T)
rf.model$confusion
rf.model$importance
p <- predict(rf.model,test)
table(p,test$Private)
rf.model$importance
install.packages('e1071')
loans <- read.csv('R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/loan_data.csv')
summary(loans)
str(loans)
loans$inq.last.6mths <- factor(loans$inq.last.6mths)
loans$delinq.2yrs <- factor(loans$delinq.2yrs)
loans$pub.rec <- factor(loans$pub.rec)
loans$not.fully.paid <- factor(loans$not.fully.paid)
loans$credit.policy <- factor(loans$credit.policy)
#EDA
library(ggplot2)
pl <- ggplot(data=loans, aes(x=fico)) + geom_histogram(aes(fill=not.fully.paid))
pl
pl <- ggplot(data=loans, aes(x=factor(purpose))) + geom_bar(aes(fill=not.fully.paid), position = 'dodge')
pl
pl <- ggplot(data=loans, aes(x=int.rate, y=fico)) + geom_point()
pl
pl <- ggplot(data=loans, aes(x=int.rate, y=fico)) + geom_point(aes(fill=not.fully.paid), alpha=0.5)
pl
#Train Test Sets
library(caTools)
sample <- sample.split(loans$not.fully.paid, SplitRatio = 0.7)
train <- subset(loans, sample==T)
test <- subset(loans, sample==F)
library(e1071)
model <- svm(not.fully.paid ~ . , data=train)
summary(model)
pred.val <- predict(model, test[1:13])
table(pred.val, test$not.fully.paid)
#tuning
tune.results <- tune(svm,train.x=not.fully.paid~., data=train,kernel='radial', ranges=list(cost=c(1,10), gamma=c(0.1,1)))
tuned.model <- svm(not.fully.paid ~ . , data=train)
pred.val <- predict(tuned.model, test[1:13])
table(pred.val, test$not.fully.paid)
tuned.model <- svm(not.fully.paid ~ . , data=train, cost=10,gamma = 0.1)
pred.val <- predict(tuned.model, test[1:13])
table(pred.val, test$not.fully.paid)
df1 <- read.csv('R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/winequality-red.csv', sep = ';')
df2 <- read.csv('R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/winequality-white.csv', sep = ';')
df1$label <- 'red'
df2$label <- 'white'
head(df1)
head(df2)
wine <- rbind(df1,df2)
str(wine)
pl <- ggplot(x=residual.sugar, data=wine) + geom_histogram(aes(fill=label))
library(ggplot2)
pl <- ggplot(x=residual.sugar, data=wine) + geom_histogram(aes(fill=label))
pl
pl <- ggplot(data=wine, x=residual.sugar) + geom_histogram(aes(fill=label))
pl
pl <- ggplot(data=wine, aes(x=residual.sugar)) + geom_histogram(aes(fill=label))
pl
pl <- ggplot(data=wine, aes(x=residual.sugar)) + geom_histogram(aes(fill=label)) + scale_fill_manual(values = c('red','white'))
pl
pl <- ggplot(data=wine, aes(x=residual.sugar)) + geom_histogram(aes(fill=label)) + scale_fill_manual(values = c('#ae4554','#faf7ea'))
pl
pl <- ggplot(data=wine, aes(x=citric.acid)) + geom_histogram(aes(fill=label)) + scale_fill_manual(values = c('#ae4554','#faf7ea'))
pl
pl <- ggplot(data=wine, aes(x=alcohol)) + geom_histogram(aes(fill=label)) + scale_fill_manual(values = c('#ae4554','#faf7ea'))
pl
pl <- ggplot(data=wine, aes(x=citric.acid, y=residual.sugar)) + geom_point(aes(fill=label), alpha=0.5) + scale_fill_manual(values = c('#ae4554','#faf7ea'))
pl
pl <- ggplot(data=wine, aes(x=citric.acid, y=residual.sugar)) + geom_point(aes(fill=label), alpha=0.5) + scale_color_manual(values = c('#ae4554','#faf7ea'))
pl
pl <- ggplot(data=wine, aes(x=citric.acid, y=residual.sugar)) + geom_point(aes(color=label), alpha=0.5) + scale_color_manual(values = c('#ae4554','#faf7ea'))
pl
pl <- ggplot(data=wine, aes(x=citric.acid, y=residual.sugar)) + geom_point(aes(color=label), alpha=0.5) + scale_color_manual(values = c('#ae4554','#faf7ea')) + theme_dark()
pl
pl <- ggplot(data=wine, aes(x=volatile.acidity, y=residual.sugar)) + geom_point(aes(color=label), alpha=0.5) + scale_color_manual(values = c('#ae4554','#faf7ea')) + theme_dark()
pl
wine[,-label]
clus.data <- wine[,1:12]
head(clus.data)
#building the clusters
wine.cluster <- kmeans(clus.data,2)
print(wine.cluster$centers)
#evaluate the clusters
table(wine.cluster$cluster, wine$label)
install.packages('tm',repos='http://cran.us.r-project.org')
install.packages('twitteR')
install.packages('wordcloud')
install.packages('RColorBrewer')
install.packages('e1017'')
install.packages('class')
install.packages('e1017')
install.packages('class')
install.packages('MASS')
head(Boston)
library(MASS)
head(Boston)
str(Boston)
any(is.na(Boston))
data <- Boston
maxs <- apply(data, 2, max)
maxs
mins <- apply(data, 2, min)
mins
scaled.data <- scale(data, center=mins, scale=maxs-mins)
scaled.data
scaled <- as.data.frame(scaled.data)
scaled
split <- sample.split(scaled$medv, SplitRatio=0.7)
library(caTools)
split <- sample.split(scaled$medv, SplitRatio=0.7)
train <- subset(scaled, split==T)
test <- subset(scaled, split==F)
install.packages('neuralnet')
library(neuralnet)
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f,data=train,hidden=c(5,3),linear.output=TRUE)
plot(nn)
predict.nn <- compute(nn, test[1:13])
str(preict.nn)
str(predict.nn)
true.predictions <- predict.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test)
MSE.nn
error.df <- data.frame(test.r,true.predictions)
library(ggplot2)
ggplot(error.df,aes(x=test.r,y=true.predictions)) + geom_point() + stat_smooth()
data <- read.csv('R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/bank_note_data.csv')
head(data)
str(data)
split <- sample.split(data, SplitRatio = 0.7)
train <- subset(data, split==T)
test <- subset(data, split==F)
str(train)
library(neuralnet)
help('neuralnet')
nn <- neuralnet(Class ~ Image.Var + Image.Skew + Image.Curt + Entropy,data=train,hidden=10,linear.output=FALSE)
pred <- compute(nn, test[1:4])
head(pred)
head(pred)
head(pred$net.result)
pred <- sapply(pred$net.result, round)
head(pred)
table(pred$net.result, test[5])
table(pred, test[5])
table(pred, test$Class)
library(randomForest)
df$Class <- factor(df$Class)
df$Class <- factor(df$Class)
data$Class <- factor(data$Class)
split = sample.split(data$Class, SplitRatio = 0.70)
train = subset(data, split == TRUE)
test = subset(data, split == FALSE)
model <- randomForest(Class ~ Image.Var + Image.Skew + Image.Curt + Entropy,data=train)
rf.pred <- predict(model,test)
table(rf.pred,test$Class)
install.packages('Tmisc')
library(Tmisc)
data("quartet")
View(quartet)
quartet %>% group_by(set) %>% summarize(mean(x),sd(x),mean(y),sd(y),cor(x,y))
install.packages()
install.packages('tidyverse')
library(tidyverse)
quartet %>% group_by(set) %>% summarize(mean(x),sd(x),mean(y),sd(y),cor(x,y))
ggplot(quratet, aes(x,y)) + geom_point() + geom_smooth(method=lm, se=FALSE) + facet_wrap(-set)
ggplot(quartet, aes(x,y)) + geom_point() + geom_smooth(method=lm, se=FALSE) + facet_wrap(-set)
ggplot(quartet, aes(x,y)) + geom_point() + geom_smooth(method=lm, se=FALSE) + facet_wrap("set")
ggplot(quartet, aes(x,y)) + geom_point() + geom_smooth(method=lm, se=FALSE) + facet_wrap(-"set")
install.packages('datasauRus')
library(datasauRus)
ggplot(datasaurus_dozen, aes(x,y,colour=dataset))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap(dataset)
ggplot(datasaurus_dozen, aes(x,y,colour='dataset'))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap('dataset')
ggplot(datasaurus_dozen, aes(x,y,colour='dataset'))+geom_point()+theme_void()+theme(legend.position = "none")+facet_wrap('dataset')
install.packages("SimDesign")
library(SimDesign)
install.packages('rmarkdown')
